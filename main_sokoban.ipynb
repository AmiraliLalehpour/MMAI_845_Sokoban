{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c47b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import time\n",
    "import copy\n",
    "import random \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51f3f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gym==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e2c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39b7761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.17 (main, Jul  5 2023, 21:22:06) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "from sokoban_env import SokobanEnv\n",
    "import sys\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1297e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Sokoban environment\n",
    "env_name = 'Sokoban-v2'\n",
    "game_env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a991565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert state to a tuple\n",
    "def state_to_tuple(state):\n",
    "    return tuple(state.reshape(-1))\n",
    "\n",
    "# Save the original state of the environment\n",
    "initial_state = game_env.reset()\n",
    "initial_state_tuple = state_to_tuple(initial_state)\n",
    "game_env.render(mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbfe8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action lookup\n",
    "ACTION_LOOKUP = game_env.unwrapped.get_action_lookup()\n",
    "# Convert state to tuple representation (for tabular SARSA)\n",
    "def state_to_tuple(state):\n",
    "    return tuple(state.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94969223",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_agent_position = game_env.player_position\n",
    "initial_box_mapping = game_env.box_mapping\n",
    "initial_room_fixed = game_env.room_fixed\n",
    "initial_room_state = game_env.room_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ffe6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_room_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a03f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_box_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bdf15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_env.box_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035845d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.utils import seeding\n",
    "from gym.spaces.discrete import Discrete\n",
    "from gym.spaces import Box\n",
    "from room_utils import generate_room\n",
    "from render_utils import room_to_rgb, room_to_tiny_world_rgb\n",
    "import numpy as np\n",
    "\n",
    "from gym.envs.registration import register\n",
    "\n",
    "from pickletools import UP_TO_NEWLINE\n",
    "from stat import UF_OPAQUE\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# np.random.seed(4)\n",
    "\n",
    "class my_sokoban_env(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array', 'tiny_human', 'tiny_rgb_array', 'raw'],\n",
    "        'render_modes': ['human', 'rgb_array', 'tiny_human', 'tiny_rgb_array', 'raw']\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim_room=(10, 10),\n",
    "                 max_steps=120,\n",
    "                 num_boxes=4,\n",
    "                 num_gen_steps=None,\n",
    "                 reset=True,\n",
    "                 random_seed=7,\n",
    "                 initial_agent_position=None,\n",
    "                 initial_box_mapping=None,\n",
    "                 initial_room_fixed=None,\n",
    "                 initial_room_state=None,\n",
    "                 second_player_added = False\n",
    "                ):\n",
    "\n",
    "        # General Configuration\n",
    "        self.dim_room = dim_room\n",
    "        if num_gen_steps == None:\n",
    "            self.num_gen_steps = int(1.7 * (dim_room[0] + dim_room[1]))\n",
    "        else:\n",
    "            self.num_gen_steps = num_gen_steps\n",
    "\n",
    "        self.num_boxes = num_boxes\n",
    "        self.boxes_on_target = 0\n",
    "\n",
    "        # Penalties and Rewards\n",
    "        self.penalty_for_step = -0.1\n",
    "        self.penalty_box_off_target = -1\n",
    "        self.reward_box_on_target = 1\n",
    "        self.reward_finished = 10\n",
    "        self.reward_last = 0\n",
    "        self.random_seed = random_seed\n",
    "        self.second_player_added = False\n",
    "\n",
    "        # Other Settings\n",
    "        self.viewer = None\n",
    "        self.max_steps = max_steps\n",
    "        self.action_space = Discrete(len(ACTION_LOOKUP))\n",
    "        screen_height, screen_width = (dim_room[0] * 16, dim_room[1] * 16)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(screen_height, screen_width, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Add new attributes for storing initial states\n",
    "        self.initial_agent_position = initial_agent_position\n",
    "        self.initial_box_mapping = initial_box_mapping\n",
    "        self.initial_room_fixed = initial_room_fixed\n",
    "        self.initial_room_state = initial_room_state\n",
    "\n",
    "    def step(self, action, observation_mode='rgb_array'):\n",
    "        assert action in ACTION_LOOKUP\n",
    "        assert observation_mode in ['rgb_array', 'tiny_rgb_array', 'raw']\n",
    "\n",
    "        self.num_env_steps += 1\n",
    "\n",
    "        self.new_box_position = None\n",
    "        self.old_box_position = None\n",
    "\n",
    "        moved_box = False\n",
    "\n",
    "        if action == 0:\n",
    "            moved_player = False\n",
    "\n",
    "        # All push actions are in the range of [0, 3]\n",
    "        elif action < 5:\n",
    "            moved_player, moved_box = self._push(action)\n",
    "\n",
    "        else:\n",
    "            moved_player = self._move(action)\n",
    "\n",
    "        self._calc_reward()\n",
    "        \n",
    "        done = self._check_if_done()\n",
    "\n",
    "        # Convert the observation to RGB frame\n",
    "        observation = self.render(mode=observation_mode)\n",
    "\n",
    "        info = {\n",
    "            \"action.name\": ACTION_LOOKUP[action],\n",
    "            \"action.moved_player\": moved_player,\n",
    "            \"action.moved_box\": moved_box,\n",
    "        }\n",
    "        if done:\n",
    "            info[\"maxsteps_used\"] = self._check_if_maxsteps()\n",
    "            info[\"all_boxes_on_target\"] = self._check_if_all_boxes_on_target()\n",
    "\n",
    "        return observation, self.reward_last, done, info\n",
    "\n",
    "    def _push(self, action):\n",
    "        \"\"\"\n",
    "        Perform a push, if a box is adjacent in the right direction.\n",
    "        If no box, can be pushed, try to move.\n",
    "        :param action:\n",
    "        :return: Boolean, indicating a change of the room's state\n",
    "        \"\"\"\n",
    "        change = CHANGE_COORDINATES[(action - 1) % 4]\n",
    "        new_position = self.player_position + change\n",
    "        current_position = self.player_position.copy()\n",
    "\n",
    "        # No push, if the push would get the box out of the room's grid\n",
    "        new_box_position = new_position + change\n",
    "        if new_box_position[0] >= self.room_state.shape[0] \\\n",
    "                or new_box_position[1] >= self.room_state.shape[1]:\n",
    "            return False, False\n",
    "\n",
    "\n",
    "        can_push_box = self.room_state[new_position[0], new_position[1]] in [3, 4]\n",
    "        can_push_box &= self.room_state[new_box_position[0], new_box_position[1]] in [1, 2]\n",
    "        if can_push_box:\n",
    "\n",
    "            self.new_box_position = tuple(new_box_position)\n",
    "            self.old_box_position = tuple(new_position)\n",
    "\n",
    "            # Move Player\n",
    "            self.player_position = new_position\n",
    "            self.room_state[(new_position[0], new_position[1])] = 5\n",
    "            self.room_state[current_position[0], current_position[1]] = \\\n",
    "                self.room_fixed[current_position[0], current_position[1]]\n",
    "\n",
    "            # Move Box\n",
    "            box_type = 4\n",
    "            if self.room_fixed[new_box_position[0], new_box_position[1]] == 2:\n",
    "                box_type = 3\n",
    "            self.room_state[new_box_position[0], new_box_position[1]] = box_type\n",
    "            return True, True\n",
    "\n",
    "        # Try to move if no box to push, available\n",
    "        else:\n",
    "            return self._move(action), False\n",
    "\n",
    "    def _move(self, action):\n",
    "        \"\"\"\n",
    "        Moves the player to the next field, if it is not occupied.\n",
    "        :param action:\n",
    "        :return: Boolean, indicating a change of the room's state\n",
    "        \"\"\"\n",
    "        change = CHANGE_COORDINATES[(action - 1) % 4]\n",
    "        new_position = self.player_position + change\n",
    "        current_position = self.player_position.copy()\n",
    "\n",
    "        # Move player if the field in the moving direction is either\n",
    "        # an empty field or an empty box target.\n",
    "        if self.room_state[new_position[0], new_position[1]] in [1, 2]:\n",
    "            self.player_position = new_position\n",
    "            self.room_state[(new_position[0], new_position[1])] = 5\n",
    "            self.room_state[current_position[0], current_position[1]] = \\\n",
    "                self.room_fixed[current_position[0], current_position[1]]\n",
    "\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _calc_reward(self):\n",
    "        \"\"\"\n",
    "        Calculate Reward Based on\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Every step a small penalty is given, This ensures\n",
    "        # that short solutions have a higher reward.\n",
    "        self.reward_last = self.penalty_for_step\n",
    "\n",
    "        # count boxes off or on the target\n",
    "        empty_targets = self.room_state == 2\n",
    "        player_on_target = (self.room_fixed == 2) & (self.room_state == 5)\n",
    "        total_targets = empty_targets | player_on_target\n",
    "\n",
    "        current_boxes_on_target = self.num_boxes - \\\n",
    "                                  np.where(total_targets)[0].shape[0]\n",
    "\n",
    "        # Add the reward if a box is pushed on the target and give a\n",
    "        # penalty if a box is pushed off the target.\n",
    "        if current_boxes_on_target > self.boxes_on_target:\n",
    "            self.reward_last += self.reward_box_on_target\n",
    "        elif current_boxes_on_target < self.boxes_on_target:\n",
    "            self.reward_last += self.penalty_box_off_target\n",
    "        \n",
    "        game_won = self._check_if_all_boxes_on_target()        \n",
    "        if game_won:\n",
    "            self.reward_last += self.reward_finished\n",
    "        \n",
    "        self.boxes_on_target = current_boxes_on_target\n",
    "\n",
    "    def _check_if_done(self):\n",
    "        # Check if the game is over either through reaching the maximum number\n",
    "        # of available steps or by pushing all boxes on the targets.        \n",
    "        return self._check_if_all_boxes_on_target() or self._check_if_maxsteps()\n",
    "\n",
    "    def _check_if_all_boxes_on_target(self):\n",
    "        empty_targets = self.room_state == 2\n",
    "        player_hiding_target = (self.room_fixed == 2) & (self.room_state == 5)\n",
    "        are_all_boxes_on_targets = np.where(empty_targets | player_hiding_target)[0].shape[0] == 0\n",
    "        return are_all_boxes_on_targets\n",
    "\n",
    "    def _check_if_maxsteps(self):\n",
    "        return (self.max_steps == self.num_env_steps)\n",
    "    \n",
    "    def reset(self, second_player=False, render_mode='rgb_array'):\n",
    "        try:\n",
    "            self.room_fixed, self.room_state, self.box_mapping = generate_room(\n",
    "                dim=self.dim_room,\n",
    "                num_steps=self.num_gen_steps,\n",
    "                num_boxes=self.num_boxes,\n",
    "                second_player=second_player,\n",
    "                \n",
    "            )\n",
    "\n",
    "        except (RuntimeError, RuntimeWarning) as e:\n",
    "            print(\"[SOKOBAN] Runtime Error/Warning: {}\".format(e))\n",
    "            print(\"[SOKOBAN] Retry . . .\")\n",
    "            return self.reset(second_player=second_player, render_mode=render_mode)\n",
    "\n",
    "        self.player_position = np.argwhere(self.room_state == 5)[0]\n",
    "        self.num_env_steps = 0\n",
    "        self.reward_last = 0\n",
    "        self.boxes_on_target = 0\n",
    "\n",
    "        starting_observation = self.render(render_mode)\n",
    "        return starting_observation\n",
    "    \n",
    "    def second_reset(self, second_player=False, render_mode='rgb_array'):\n",
    "        try:\n",
    "\n",
    "#             self.room_fixed, self.room_state, self.box_mapping = initial_room_fixed,initial_room_state, initial_box_mapping\n",
    "            # Reset the environment to the initial states\n",
    "            self.player_position = self.initial_agent_position.copy()\n",
    "            self.box_mapping = self.initial_box_mapping.copy()\n",
    "            self.room_fixed = self.initial_room_fixed.copy()\n",
    "            self.room_state = self.initial_room_state.copy()\n",
    "\n",
    "        except (RuntimeError, RuntimeWarning) as e:\n",
    "            print(\"[SOKOBAN] Runtime Error/Warning: {}\".format(e))\n",
    "            print(\"[SOKOBAN] Retry . . .\")\n",
    "            return self.reset(second_player=second_player, render_mode=render_mode)\n",
    "\n",
    "        self.player_position = np.argwhere(self.room_state == 5)[0]\n",
    "        self.num_env_steps = 0\n",
    "        self.reward_last = 0\n",
    "        self.boxes_on_target = 0\n",
    "\n",
    "        starting_observation = self.render(render_mode)\n",
    "        return starting_observation\n",
    "\n",
    "    def render(self, mode='human', close=None, scale=1):\n",
    "        assert mode in RENDERING_MODES\n",
    "\n",
    "        img = self.get_image(mode, scale)\n",
    "\n",
    "        if 'rgb_array' in mode:\n",
    "            return img\n",
    "\n",
    "        elif 'human' in mode:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            if self.viewer is None:\n",
    "                self.viewer = rendering.SimpleImageViewer()\n",
    "            self.viewer.imshow(img)\n",
    "            return self.viewer.isopen\n",
    "\n",
    "        elif 'raw' in mode:\n",
    "            arr_walls = (self.room_fixed == 0).view(np.int8)\n",
    "            arr_goals = (self.room_fixed == 2).view(np.int8)\n",
    "            arr_boxes = ((self.room_state == 4) + (self.room_state == 3)).view(np.int8)\n",
    "            arr_player = (self.room_state == 5).view(np.int8)\n",
    "\n",
    "            return arr_walls, arr_goals, arr_boxes, arr_player\n",
    "\n",
    "        else:\n",
    "            super(SokobanEnv, self).render(mode=mode)  # just raise an exception\n",
    "\n",
    "    def get_image(self, mode, scale=1):\n",
    "        \n",
    "        if mode.startswith('tiny_'):\n",
    "            img = room_to_tiny_world_rgb(self.room_state, self.room_fixed, scale=scale)\n",
    "        else:\n",
    "            img = room_to_rgb(self.room_state, self.room_fixed)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.close()\n",
    "\n",
    "    def set_maxsteps(self, num_steps):\n",
    "        self.max_steps = num_steps\n",
    "\n",
    "    def get_action_lookup(self):\n",
    "        return ACTION_LOOKUP\n",
    "\n",
    "    def get_action_meanings(self):\n",
    "        return ACTION_LOOKUP\n",
    "\n",
    "ACTION_LOOKUP = {\n",
    "    0: 'no operation',\n",
    "    1: 'push up',\n",
    "    2: 'push down',\n",
    "    3: 'push left',\n",
    "    4: 'push right',\n",
    "    5: 'move up',\n",
    "    6: 'move down',\n",
    "    7: 'move left',\n",
    "    8: 'move right',\n",
    "}\n",
    "\n",
    "# Moves are mapped to coordinate changes as follows\n",
    "# 0: Move up\n",
    "# 1: Move down\n",
    "# 2: Move left\n",
    "# 3: Move right\n",
    "CHANGE_COORDINATES = {\n",
    "    0: (-1, 0),\n",
    "    1: (1, 0),\n",
    "    2: (0, -1),\n",
    "    3: (0, 1)\n",
    "}\n",
    "\n",
    "RENDERING_MODES = ['rgb_array', 'human', 'tiny_rgb_array', 'tiny_human', 'raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa936d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rl_sarsa_sokoban\n",
    "# from custom_sokoban_env import my_sokoban_env\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "\n",
    "# SARSA Algorithm\n",
    "\n",
    "num_episodes= 500\n",
    "alpha=0.3\n",
    "gamma=1\n",
    "epsilon=0.05\n",
    "\n",
    "q_table = {} \n",
    "\n",
    "# Training the agent\n",
    "game_env = my_sokoban_env(initial_agent_position=initial_agent_position,\n",
    "    initial_box_mapping=initial_box_mapping,\n",
    "    initial_room_fixed=initial_room_fixed,\n",
    "    initial_room_state=initial_room_state)\n",
    "# SARSA algorithm\n",
    "for episode in range(num_episodes):\n",
    "\n",
    "    print('starting episode', episode+1)\n",
    "    \n",
    "    \n",
    "    state = game_env.second_reset() \n",
    "    # Update the initial variables with the current state returned by second_reset()\n",
    "    initial_agent_position = game_env.initial_agent_position\n",
    "    initial_box_mapping = game_env.initial_box_mapping\n",
    "    initial_room_fixed = game_env.initial_room_fixed\n",
    "    initial_room_state = game_env.initial_room_state\n",
    "    print(initial_box_mapping)\n",
    "    state_tuple = state_to_tuple(state)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    # Initialize Q-values for the current state if not present\n",
    "    if state_tuple not in q_table:\n",
    "        q_table[state_tuple] = np.zeros(game_env.action_space.n)\n",
    "\n",
    "    # Choose the initial action based on epsilon-greedy policy\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = game_env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(q_table[state_tuple])\n",
    "\n",
    "    while not done:\n",
    "#         game_env.render(mode='human')\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Take the chosen action\n",
    "        next_state, reward, done, _ = game_env.step(action)\n",
    "        next_state_tuple = state_to_tuple(next_state)\n",
    "\n",
    "        # Initialize Q-values for the next state if not present\n",
    "        if next_state_tuple not in q_table:\n",
    "            q_table[next_state_tuple] = np.zeros(game_env.action_space.n)\n",
    "\n",
    "        # Choose the next action based on epsilon-greedy policy\n",
    "        if np.random.rand() < epsilon:\n",
    "            next_action = game_env.action_space.sample()\n",
    "        else:\n",
    "            next_action = np.argmax(q_table[next_state_tuple])\n",
    "\n",
    "        # SARSA Q-value update\n",
    "        q_value = q_table[state_tuple][action]\n",
    "        next_q_value = q_table[next_state_tuple][next_action]\n",
    "        q_table[state_tuple][action] = q_value + alpha * (reward + gamma * next_q_value - q_value)\n",
    "\n",
    "        state = next_state.copy()  # Copy the next_state into the state variable\n",
    "        state_tuple = next_state_tuple\n",
    "        action = next_action\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            print(\"Episode: {}, Total Reward: {}\".format(episode + 1, total_reward))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6469129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function to test the agent's performance in one final test episode using the Q-table\n",
    "def test_agent(q_table, game_env):\n",
    "    state = game_env.second_reset() \n",
    "    # Update the initial variables with the current state returned by second_reset()\n",
    "    initial_agent_position = game_env.initial_agent_position\n",
    "    initial_box_mapping = game_env.initial_box_mapping\n",
    "    initial_room_fixed = game_env.initial_room_fixed\n",
    "    initial_room_state = game_env.initial_room_state\n",
    "    print(initial_box_mapping)\n",
    "    state_tuple = state_to_tuple(state)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Choose the best action based on the Q-table\n",
    "        action = np.argmax(q_table[state_tuple])\n",
    "\n",
    "        next_state, reward, done, _ = game_env.step(action)\n",
    "        next_state_tuple = state_to_tuple(next_state)\n",
    "\n",
    "        state = next_state.copy()\n",
    "        state_tuple = next_state_tuple\n",
    "        total_reward += reward\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "# Initialize the game environment\n",
    "game_env = my_sokoban_env(initial_agent_position=initial_agent_position,\n",
    "                          initial_box_mapping=initial_box_mapping,\n",
    "                          initial_room_fixed=initial_room_fixed,\n",
    "                          initial_room_state=initial_room_state)\n",
    "\n",
    "# Test the agent using the trained Q-table for one final episode\n",
    "final_reward = test_agent(q_table, game_env)\n",
    "\n",
    "# Print the final reward obtained in the test episode\n",
    "print(\"Final Test Episode Reward:\", final_reward)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
