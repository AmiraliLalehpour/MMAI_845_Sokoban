{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34dd8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import time\n",
    "from sokoban_env import SokobanEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8cd260a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28d5127",
   "metadata": {},
   "source": [
    "# Q-learning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4213f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Sokoban environment\n",
    "env = SokobanEnv(dim_room=(10, 10), num_boxes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29c55f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "env.render(mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b41aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action lookup\n",
    "ACTION_LOOKUP = env.unwrapped.get_action_lookup()\n",
    "# Convert state to tuple representation (for tabular SARSA)\n",
    "def state_to_tuple(state):\n",
    "    return tuple(state.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e63c92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: -40.10000000000006\n",
      "Episode: 2, Total Reward: -44.10000000000005\n",
      "Episode: 3, Total Reward: -33.10000000000006\n",
      "Episode: 4, Total Reward: -77.60000000000005\n",
      "Episode: 5, Total Reward: -56.300000000000054\n",
      "Episode: 6, Total Reward: -77.60000000000005\n",
      "Episode: 7, Total Reward: -51.60000000000006\n",
      "Episode: 8, Total Reward: -18.600000000000037\n",
      "Episode: 9, Total Reward: -57.100000000000044\n",
      "Episode: 10, Total Reward: -57.10000000000005\n",
      "Episode: 11, Total Reward: -73.30000000000004\n",
      "Episode: 12, Total Reward: -34.80000000000005\n",
      "Episode: 13, Total Reward: -57.10000000000005\n",
      "Episode: 14, Total Reward: -55.600000000000044\n",
      "Episode: 15, Total Reward: -57.10000000000005\n",
      "Episode: 16, Total Reward: -58.300000000000054\n",
      "Episode: 17, Total Reward: -56.30000000000005\n",
      "Episode: 18, Total Reward: -53.10000000000005\n",
      "Episode: 19, Total Reward: -73.30000000000005\n",
      "Episode: 20, Total Reward: -56.10000000000005\n",
      "Episode: 21, Total Reward: -56.10000000000005\n",
      "Episode: 22, Total Reward: -52.10000000000005\n",
      "Episode: 23, Total Reward: -56.30000000000005\n",
      "Episode: 24, Total Reward: -56.10000000000005\n",
      "Episode: 25, Total Reward: -56.10000000000005\n",
      "Episode: 26, Total Reward: -57.100000000000044\n",
      "Episode: 27, Total Reward: -57.30000000000005\n",
      "Episode: 28, Total Reward: 4.339999999999968\n",
      "Episode: 29, Total Reward: -73.10000000000004\n",
      "Episode: 30, Total Reward: -57.10000000000005\n",
      "Episode: 31, Total Reward: -56.10000000000005\n",
      "Episode: 32, Total Reward: -5.620000000000022\n",
      "Episode: 33, Total Reward: -56.30000000000005\n",
      "Episode: 34, Total Reward: -22.60000000000005\n",
      "Episode: 35, Total Reward: -23.100000000000048\n",
      "Episode: 36, Total Reward: -74.10000000000005\n",
      "Episode: 37, Total Reward: -56.30000000000005\n",
      "Episode: 38, Total Reward: 19.33999999999999\n",
      "Episode: 39, Total Reward: -38.10000000000005\n",
      "Episode: 40, Total Reward: -56.100000000000044\n",
      "Episode: 41, Total Reward: -56.100000000000044\n",
      "Episode: 42, Total Reward: 27.65999999999999\n",
      "Episode: 43, Total Reward: -56.30000000000005\n",
      "Episode: 44, Total Reward: -56.10000000000005\n",
      "Episode: 45, Total Reward: -57.100000000000044\n",
      "Episode: 46, Total Reward: -31.86000000000005\n",
      "Episode: 47, Total Reward: -31.80000000000006\n",
      "Episode: 48, Total Reward: -57.10000000000005\n",
      "Episode: 49, Total Reward: 30.21999999999999\n",
      "Episode: 50, Total Reward: -31.10000000000005\n",
      "Episode: 51, Total Reward: 31.499999999999993\n",
      "Episode: 52, Total Reward: -57.100000000000044\n",
      "Episode: 53, Total Reward: 34.69999999999999\n",
      "Episode: 54, Total Reward: 34.69999999999999\n",
      "Episode: 55, Total Reward: 34.69999999999999\n",
      "Episode: 56, Total Reward: 34.69999999999999\n",
      "Episode: 57, Total Reward: 34.69999999999999\n",
      "Episode: 58, Total Reward: 34.69999999999999\n",
      "Episode: 59, Total Reward: 34.69999999999999\n",
      "Episode: 60, Total Reward: 34.69999999999999\n",
      "Episode: 61, Total Reward: 34.69999999999999\n",
      "Episode: 62, Total Reward: 34.69999999999999\n",
      "Episode: 63, Total Reward: 34.69999999999999\n",
      "Episode: 64, Total Reward: 34.69999999999999\n",
      "Episode: 65, Total Reward: 34.69999999999999\n",
      "Episode: 66, Total Reward: 34.69999999999999\n",
      "Episode: 67, Total Reward: 34.69999999999999\n",
      "Episode: 68, Total Reward: 34.69999999999999\n",
      "Episode: 69, Total Reward: 34.69999999999999\n",
      "Episode: 70, Total Reward: 34.69999999999999\n",
      "Episode: 71, Total Reward: 34.69999999999999\n",
      "Episode: 72, Total Reward: 34.69999999999999\n",
      "Episode: 73, Total Reward: 34.69999999999999\n",
      "Episode: 74, Total Reward: 34.69999999999999\n",
      "Episode: 75, Total Reward: 34.69999999999999\n",
      "Episode: 76, Total Reward: 34.69999999999999\n",
      "Episode: 77, Total Reward: 34.69999999999999\n",
      "Episode: 78, Total Reward: 34.69999999999999\n",
      "Episode: 79, Total Reward: 34.69999999999999\n",
      "Episode: 80, Total Reward: 34.69999999999999\n",
      "Episode: 81, Total Reward: 34.69999999999999\n",
      "Episode: 82, Total Reward: 34.69999999999999\n",
      "Episode: 83, Total Reward: 34.69999999999999\n",
      "Episode: 84, Total Reward: 34.69999999999999\n",
      "Episode: 85, Total Reward: 34.69999999999999\n",
      "Episode: 86, Total Reward: 34.69999999999999\n",
      "Episode: 87, Total Reward: 34.69999999999999\n",
      "Episode: 88, Total Reward: 34.69999999999999\n",
      "Episode: 89, Total Reward: 34.69999999999999\n",
      "Episode: 90, Total Reward: 34.69999999999999\n",
      "Episode: 91, Total Reward: 34.69999999999999\n",
      "Episode: 92, Total Reward: 34.69999999999999\n",
      "Episode: 93, Total Reward: 34.69999999999999\n",
      "Episode: 94, Total Reward: 34.69999999999999\n",
      "Episode: 95, Total Reward: 34.55999999999999\n",
      "Episode: 96, Total Reward: 34.69999999999999\n",
      "Episode: 97, Total Reward: 34.69999999999999\n",
      "Episode: 98, Total Reward: 34.69999999999999\n",
      "Episode: 99, Total Reward: 34.69999999999999\n",
      "Episode: 100, Total Reward: 34.69999999999999\n",
      "Episode: 101, Total Reward: 34.69999999999999\n",
      "Episode: 102, Total Reward: 34.69999999999999\n",
      "Episode: 103, Total Reward: 34.69999999999999\n",
      "Episode: 104, Total Reward: 34.69999999999999\n",
      "Episode: 105, Total Reward: 34.69999999999999\n",
      "Episode: 106, Total Reward: 34.69999999999999\n",
      "Episode: 107, Total Reward: 34.69999999999999\n",
      "Episode: 108, Total Reward: 34.69999999999999\n",
      "Episode: 109, Total Reward: 34.69999999999999\n",
      "Episode: 110, Total Reward: 34.05999999999999\n",
      "Episode: 111, Total Reward: 34.69999999999999\n",
      "Episode: 112, Total Reward: 34.69999999999999\n",
      "Episode: 113, Total Reward: 33.999999999999986\n",
      "Episode: 114, Total Reward: 34.69999999999999\n",
      "Episode: 115, Total Reward: 34.69999999999999\n",
      "Episode: 116, Total Reward: 34.69999999999999\n",
      "Episode: 117, Total Reward: 34.69999999999999\n",
      "Episode: 118, Total Reward: 34.69999999999999\n",
      "Episode: 119, Total Reward: 34.69999999999999\n",
      "Episode: 120, Total Reward: 34.69999999999999\n",
      "Episode: 121, Total Reward: 34.69999999999999\n",
      "Episode: 122, Total Reward: 34.69999999999999\n",
      "Episode: 123, Total Reward: 34.69999999999999\n",
      "Episode: 124, Total Reward: 34.69999999999999\n"
     ]
    }
   ],
   "source": [
    "# SARSA parameters\n",
    "num_episodes = 500\n",
    "learning_rate = 0.5\n",
    "discount_factor = 0.99\n",
    "exploration_prob = 0.01\n",
    "\n",
    "# Q-table initialization\n",
    "q_table = {}\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    state_tuple = state_to_tuple(state)\n",
    "    total_reward = 0\n",
    "\n",
    "    # Initialize Q-values for the current state if not present\n",
    "    if state_tuple not in q_table:\n",
    "        q_table[state_tuple] = np.zeros(env.action_space.n)\n",
    "\n",
    "    while not done:\n",
    "        env.render(mode='human')\n",
    "        time.sleep(0.01)\n",
    "\n",
    "        # Choose the action based on epsilon-greedy policy\n",
    "        if np.random.rand() < exploration_prob:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state_tuple])\n",
    "\n",
    "        # Take the chosen action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state_tuple = state_to_tuple(next_state)\n",
    "\n",
    "        # Initialize Q-values for the next state if not present\n",
    "        if next_state_tuple not in q_table:\n",
    "            q_table[next_state_tuple] = np.zeros(env.action_space.n)\n",
    "\n",
    "        # Q-learning Q-value update\n",
    "        q_value = q_table[state_tuple][action]\n",
    "        max_next_q_value = np.max(q_table[next_state_tuple])\n",
    "        q_table[state_tuple][action] = q_value + learning_rate * (reward + discount_factor * max_next_q_value - q_value)\n",
    "\n",
    "        state = next_state.copy()  # Copy the next_state into the state variable\n",
    "        state_tuple = next_state_tuple\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            print(\"Episode: {}, Total Reward: {}\".format(episode + 1, total_reward))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf9156c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
